---
layout: post
title: Maximize the Likelihood Estimate
tags: [MLDL]
permalink: /2023-spring/mldl/mle
---

# MLE

# Bayes Rule

$$
P(p\|\mathcal{D}) = \frac{P(\mathcal{D}\|p)P(p)}{P(\mathcal{D})}
$$

- $$p$$ is a model and $$\mathcal{D}$$ is a data
- $$P(\mathcal{D}\|p)$$: likelihood function → what we know
  - The probability of model to produce a data
  - If $$p$$ produces $$\mathcal{D}$$ for 60%, then $$P(\mathcal{D}\|p) =0.6$$
- $$P(p)$$: prior → probability of choosing model $$p$$ (may or may not know)
  - The probability of choosing model $$p$$
  - If not given, MLE is used ↔ If given, MAP is used
- $$P(p\|\mathcal{D})$$: posterior → what we want to maximize
  - Probability of a data $$\mathcal{D}$$ is produced by a model $$p$$
  - if there are 10 $$\mathcal{D}$$s and 4 of them are from $$p_1$$ and others are from $$p_2$$. then, $$P(p_1\|D) = 0.4$$
- $$P(\mathcal{D})$$: a constant that normalizes the probability (unimportant)
- MLE: $$P(p\|\mathcal{D}) \propto P(\mathcal{D}\|p)$$ ($$P(p)$$ is unknown, black box)
- MAP: $$P(p\|\mathcal{D})\propto P(\mathcal{D}\|p)P(p)$$

## MLE: Maximum Likelihood Estimator

- As we don’t know $$P(p)$$, a prior knowledge, assume $$P(p\|\mathcal{D}) \propto P(D\|p)$$
- Then, maximizing a likelihood function $$P(D\|p)$$ leads to maximum $$P(p\|D)$$
- It means that we choose $$\theta$$ that maximizes probability of observed data
  - If $$p$$ produces $$\mathcal{D}$$ a lot, then the higher chance of $$\mathcal{D}$$ to be generated from $$p$$

$$
\hat{\theta}^{MLE} = \arg\max_\theta P(\mathcal{D}\|\theta)
$$

## MAP: Maximize a Posteriori

![Untitled](MLE%20fb436eb9659a425093e4aed1d864b3e6/Untitled.png)

- If we have prior knowledge, $$P(p)$$, then we can compute posterior
  - If we don’t have prior, than assume it is uniformly random thus it reduces to MLE problem (constant probability $$k$$)
- With prior and observation, we can update the prior that reflects the observation, which is posterior

# Toss a Coin

- Let’s start with estimating a probability of a coin
- A coin shows head for $$\alpha_H$$ times and tail for $$\alpha_T$$ times
- We know that probability of showing head $$k$$ times in $$n$$ tosses is $$p=\frac{k}{n}$$
  - But why? → we will show that $$p=\frac{k}{n}$$ is the most reasonable value

## MLE

- Assume there is no prior knowledge, that is, we don’t know that $$P(X=1)=P(X=0) = 0.5$$
- $$X\sim \text{Bernoulli}:P(X)= p^X(1-p)^{1-X}$$ (binomial)
- Flips are independent and identically distributed (i.i.d.)
- We will estimate $$p$$ by maximizing likelihood ($$P(\mathcal{D}\|p)$$)
  - Data $$\mathcal{D}$$ : observed set $$\mathcal{D}$$ of $$\alpha_H$$ and $$\alpha_T$$
  - Hypothesis space : binomial distribuiton
  - Learning: finding $$p$$ by solving optimization problem
    - Objective function:
    $$
    P(\mathcal{D}\|p)= \binom{\alpha_H+\alpha_T}{\alpha_T}p^{\alpha_H}(1-p)^{\alpha_T}
    $$
  - MLE: choose $$p$$ to maximize probability of $$\mathcal{D}$$
    $$
    \hat{p} = \arg\max_p P(\mathcal{D}\|p) = \arg\max_p \log P(\mathcal{D}\|p)
    $$
    - As $$\log$$ is a increasing function, it does not affect on $$\hat{p}$$
    - If $$P(\mathcal{D}\|p)$$ is concave, then take $$-$$ or $$-\log$$ to make convex
    - We use $$\log$$ to compute sum instead of product, and it is numerically stable
- Now let’s solve our optimization problem

$$
\begin{align*}
\arg\max_p(P(\mathcal{D}\|p) &= \arg\max_i \log\binom{\alpha_H+\alpha_T}{\alpha_H}p^{\alpha_H}(1-p)^{\alpha_T}\\
&= \arg\max_i(\log\binom{\alpha_H+\alpha_T}{\alpha_H} +\alpha_H\log p+\alpha_T\log(1-p))\\
&= \arg\max_p (\alpha_H\log p+\alpha_T\log(1-p))\\
&= \Phi(p)

\end{align*}
$$

- To find $$\hat{p}$$, we will find minimum point of $$\Phi(p)$$ with $$\frac{d\Phi(p)}{dp} = 0$$
  $$
  \begin{align*}
  &\Rightarrow 0=\frac{d\Phi(p)}{dp} = \frac{\alpha_H}{p} + \frac{\alpha_T}{1-p}\\
  &\Rightarrow (1-p)\alpha_H=p\alpha_T\\
  &\Rightarrow \alpha_H-p\alpha_H=p\alpha_T\\
  &\Rightarrow \hat{p} = \frac{\alpha_H}{\alpha_H+\alpha_T} = \frac{k}{n}
  \end{align*}
  $$
  → The result is consistent with the frequency of showing heads
- Hoeffding’s inequality shows that probability of mistake exponentially decrease as $$N$$ increases
  $$
  P(\|\hat{p}-p^*\|\ge \epsilon)\le 2e^{-2N\epsilon^2}\le 1-\delta
  $$
  - If we want to know $$p$$ within $$\epsilon = 0.1$$, with probability at least $$\delta=0.95$$,
  - $$\ln(1-\delta)\ge \ln2 - 2N\epsilon^2 \rightarrow N\ge \frac{\ln(2/(1-\delta))}{2\epsilon^2} \approx 190$$, so 190 flips are needed
